{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algebra cheat sheet (Part III)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Descriptive Geometry\n",
    "\n",
    "## Norms\n",
    "\n",
    "**(Def) Norm:** A norm on a vector space $V$ is a function $\\| \\cdot \\|:V \\rightarrow \\mathbb{R}$ such that:\n",
    "\n",
    "$\\forall \\lambda \\in \\mathbb{R}$ and $x,y \\in \\mathbb{R}$\n",
    "\n",
    "- $ \\| \\lambda c\\| = \\lambda \\|x\\| $ \n",
    "- $ \\| x + y \\| \\leq \\|x\\| + \\|y\\| $\n",
    "- $ \\| x \\| \\geq 0$ and ($\\|x\\|=0 \\iff x=0)$\n",
    "\n",
    "> **Euclidean norm**\n",
    "> $$\\| x \\|_2 = \\sqrt{\\sum_{i=1}^{n} x_{i}^2} = \\sqrt{x^T x}$$\n",
    "\n",
    "## Inner Product\n",
    "**(Def) Bilinear:** $\\Omega: V \\times V \\rightarrow \\mathbb{R}$ is bilinear if:\n",
    "\n",
    "$\\forall \\lambda, \\psi \\in \\mathbb{R}$ and $\\forall x,y,x \\in V$\n",
    "\n",
    "- $\\Omega(\\lambda x + \\psi y, z) = \\lambda \\Omega( x , z)   +  \\psi  \\Omega( y, z)$\n",
    "- $\\Omega(z , \\lambda x + \\psi y) = \\lambda \\Omega( z, x)   +  \\psi  \\Omega( z, y)$\n",
    "\n",
    "**(Def) Symmetric:** $\\Omega$ is symmetric if it is:  **bilinear** and $\\Omega(x,y) = \\Omega(y,x)$\n",
    "\n",
    "**(Def) Positive:** $\\Omega$ is positive defined if \n",
    "\n",
    "- $\\forall x \\in V \\setminus \\{0\\}$ $\\Omega(x,x) >  0$\n",
    "- $\\Omega(0, 0) = 0$\n",
    "\n",
    "**(Def) Inner-Product**\n",
    "\n",
    "$\\Omega:V \\times V \\rightarrow \\mathbb{R}$ is an inner-product if: It is linear, symmetric and positive defined.\n",
    "\n",
    "## Inner Vector Spaces\n",
    "\n",
    "The pair $(V, <\\cdot,\\cdot>)$ is an inner vector-space if $V$ is a vector space and $<\\cdot,\\cdot>$ is an inner-product.\n",
    "\n",
    "> **Euclidean Vector Space:** is a vector space with the following inner-product:\n",
    ">  $$<x,y> = x^T y = \\sum_{i=1}^{n} x_{i} y_{i} $$\n",
    "\n",
    "## Symmetric Positive Matrices\n",
    "\n",
    "Consider $(V, <\\cdot, \\cdot>)$ and $B_n[V]$ then:\n",
    "\n",
    "$$<x,y> = < \\sum_{i=1}^{n} \\lambda_i b_i  ~~,~~ \\sum_{j=1}^{n} \\psi_j b_i>$$\n",
    "\n",
    "Because of the Bilinear Property:\n",
    "\n",
    "$$<x,y> = < \\sum_{i=1}^{n} \\psi_i b_i  ~~,~~ \\sum_{j=1}^{n} \\lambda_j b_i> =  \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\psi_i <b_i, b_j> \\lambda_j$$\n",
    "\n",
    "Then lets define $A_{ij} = <b_i, b_j>$\n",
    "\n",
    "$$<x,y> =  x_{B}^T A y_B$$\n",
    "\n",
    "> This means that $<\\cdot, \\cdot>$ is unique defined by  $A_{ij} = <b_i, b_j>$,  \n",
    "> also the symmetry of $<\\cdot, \\cdot>$ implies that $A$ is symmetric and $\\forall x \\in V \\setminus \\{0\\} : x^T A x > 0$\n",
    "\n",
    "**(Def) Positive Definite** A symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ is called **positive-definite** if: $\\forall x \\in V \\setminus \\{0\\} : x^T A x > 0$\n",
    "\n",
    "> **(Theorem) All inner-product can be expressed with symmetric positive-definite matrices**\n",
    ">\n",
    "> For all $B_n[V]$ it holds that  $<\\cdot, \\cdot>:V \\times V \\rightarrow \\mathbb{R}$ is an inner-product \n",
    ">\n",
    "> There exist a symmetric positive-definite matrix $A \\in \\mathbb{R}^{n \\times n}$ with: $<x,y>= x_{b}^T A y_B$\n",
    "\n",
    "## Lengths and Distances\n",
    "\n",
    "Each inner-product induces a norm: $\\| x \\| = \\sqrt{ <x,x>}$\n",
    "\n",
    "**(Lemma) Cauchy-Schwartz Inequality:** Given a inner vector-space $(V, <\\cdot, \\cdot>)$, the induced norm $\\| \\cdot \\|$ satisfies:\n",
    "\n",
    "$$ | <x, y> | \\leq \\|x\\| \\|y\\| $$\n",
    "\n",
    "> From this point $\\| \\cdot \\|$ is the induced norm (unless specified)\n",
    "\n",
    "**(Def) Distance:** Consider $(V, <\\cdot, \\cdot>)$ the distance between $x$ and $y$ is:\n",
    "\n",
    "$$ d(x,y) = \\| x - y \\| = \\sqrt{<x-y, x-y>} $$\n",
    "\n",
    "**(Properties)** Consider $(V, <\\cdot, \\cdot>)$\n",
    "\n",
    " - $d(x,y) \\geq 0$  $\\forall x,y \\in V$\n",
    " - $d(x,y) \\geq 0$  $\\iff x=y$\n",
    " - $d(x,y) = d(y, x)$  $\\forall x,y \\in V$\n",
    " - $d(x,z) \\leq d(x,y) + d(y,z)$  $\\forall x,y,z \\in V$\n",
    "\n",
    "\n",
    "## Orthogonality\n",
    "\n",
    "Inner product also capture the geometry of a vector space by defining the angle $w$ between two vectors.\n",
    "\n",
    "$\\forall x,y \\in V \\setminus \\{0\\} ~~~ -1 \\leq  \\frac{<x,y>}{\\|x\\| \\|y\\|}  \\leq 1$\n",
    "\n",
    "And: $cos(w) = \\frac{<x,y>}{\\|x\\| \\|y\\|}$ where $w$ is the angle between $x$ and $y$\n",
    "\n",
    "**(Def) Orthogonality:** Given $(V, <\\cdot, \\cdot>)$ and $x,y \\in V$ are orthogonal **iff** $<x,y>=0$ denoted: $x \\perp y$\n",
    "\n",
    "**(Def) Orthonormal:** If $x,y \\in V$, they are orthonormal **iff** $<x,y>=0$ and $\\|x\\| = \\|y\\| = 1$ \n",
    "\n",
    "> The vector '*0*' is orthogonal to all vectors\n",
    "\n",
    "**(Def) Orthogonal Matrix:**\n",
    "\n",
    "A square matrix $A \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix **iff** $AA^T = I = A^{T}A$ which implies: $A^T = A^{-1}$\n",
    "\n",
    "> Transformation done by orthogonal matrices **do not change vector length**: \n",
    ">\n",
    "> $$\\|x\\|^2 = (Ax)^t (Ax) = x^T A^T Ax = x^T I x = x^T x = \\|x\\|^2 $$\n",
    ">\n",
    "> It turns out that orthogonal matrices define **rotations**\n",
    "\n",
    "**(Def) Orthogonal Basis:** Consider B_n[V] then $B$ is orthogonal if $\\forall i,j \\in 1, \\cdots, n$:\n",
    "\n",
    "- $<b_i,b_j>=0$ for $i \\neq j$\n",
    "- $<b_i,b_i>=1$ \n",
    "\n",
    "\n",
    "## Orthogonal Complement\n",
    "\n",
    "Consider $(V, <\\cdot, \\cdot>)$ with dimension $D$, and $U \\subset V$ a subspace with dimension M.\n",
    "\n",
    "Then $U^{\\perp}$ is an orthogonal complement of U defined as $U^{\\perp} = \\{ v \\in V | \\forall u \\in U ~~~ v \\perp u \\}$\n",
    "\n",
    "And it has the following properties:\n",
    "\n",
    "- $U^{\\perp}$ is $(D-M)-dimensional$ \n",
    "- $U \\cap U^{\\perp} = \\{ 0 \\} $\n",
    "\n",
    "**(Lema) Basis of Orthogonal Complements** \n",
    "\n",
    "Consider $(V, <\\cdot, \\cdot>)$ with dimension $D$, and $U \\subset V$ a subspace with dimension M.\n",
    "\n",
    "And $B_{D}[U],~~ B_{D-M}^{\\perp}[U^{\\perp}]$\n",
    "\n",
    "Then:\n",
    "\n",
    "$\\forall x \\in V$, $x$ can be uniquely decomposed into:\n",
    "\n",
    "$$x = \\sum_{i=1}^{M} \\lambda_i b_i ~~+~~ \\sum_{j=1}^{D-M} \\psi_j b^{\\perp}_j$$\n",
    "\n",
    "## Projections\n",
    "\n",
    "**(Def) Projection** \n",
    "\n",
    "Let $V$ be a vector space and U a sub-space. \n",
    "\n",
    "A linear mapping $\\pi : V \\rightarrow U$ is called a **projection** if: $\\pi^2 = \\pi \\circ \\pi = \\pi$ which means: $\\forall x \\in V~~ \\pi(\\pi(v))=\\pi(v)$\n",
    "\n",
    "**(Def) Projection Matrix:** \n",
    "\n",
    "Since a linear map $\\pi$ can be expressed as a matrix $A_{\\pi}$\n",
    "\n",
    "$A_{\\pi}$ is a **projection matrix** if $A_{\\pi} A_{\\pi} = A_{\\pi}$\n",
    "\n",
    "### Projections into one-dimensional subspaces (line)\n",
    "\n",
    "Consider $C_{n}[V]$ and $B_1[U]$ and $B$ is subspace of $V$\n",
    "\n",
    "We want to calculate the projection $ \\pi_{U}: V \\rightarrow U $:\n",
    "\n",
    "1) The first property we want of $\\pi$ is minimal distance:\n",
    " \n",
    " $\\pi_U(x)$ is closest to x: $\\| x - \\pi_U(x)\\|$ is minimal $\\Rightarrow$ $ x - \\pi_U(x) $ is orthogonal t $U$\n",
    "\n",
    " $\\Rightarrow$ $<\\pi_U(x) - x, b> = 0$\n",
    "\n",
    "2) The element $\\pi_U(x) \\in U$ $\\Rightarrow$ $\\pi_U(x) = \\lambda b$ for some $\\lambda \\in \\mathbb{R}$\n",
    "\n",
    "3) By finding $\\lambda$ we can find $\\pi_U$\n",
    "\n",
    "$<x - \\pi_U(x), b> =0 ~~\\iff~~<x-\\lambda b, b> =0 ~~\\iff~~ <x,b> - \\lambda <b,b> = 0 ~~\\iff~~ \\lambda = \\frac{<x,b>}{<b,b>} = \\frac{<x,b>}{\\|v\\|^2}$\n",
    "\n",
    "$\\iff \\lambda = \\frac{b^T x}{\\|v\\|^2} $\n",
    "\n",
    "So: $\\pi_U(x) = \\lambda b = \\frac{b^T x}{\\|v\\|^2} b$\n",
    "\n",
    "Now to find the projection matrix:\n",
    "\n",
    "$\\pi_U(x) = \\lambda b =  b [\\lambda] = b \\frac{b^T x}{\\|v\\|^2}  = \\frac{b b^T}{\\|v\\|^2} x$\n",
    "\n",
    "Then $A_{\\pi_U} = \\frac{b b^T}{\\|v\\|^2}$\n",
    "\n",
    "\n",
    "\n",
    "### Projections into General Sub-spaces\n",
    "\n",
    "We can generalize the previous method for project in any sub-space.\n",
    "\n",
    "Consider  $(V, <\\cdot, \\cdot>)$ and $U \\subset \\mathbb{R}^n$ a subspace of $V$ with $dim(U) = m, ~~m \\geq 1$. Also B_m[U].\n",
    "\n",
    "We want to find the orthogonal projection $\\pi_U:V \\rightarrow U$\n",
    "\n",
    "$\\pi_U(x) = \\sum_{i=1}^{m} \\lambda_i b_i = \n",
    "\\begin{bmatrix}\n",
    "b_{1,1} & b_{1,2} & \\cdots & b_{1,m} \\\\\n",
    "b_{2,1} & b_{2,2} & \\cdots & b_{2,m} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "b_{n,1} & b_{m,2} & \\cdots & b_{n,m} \n",
    "\\end{bmatrix} \\begin{bmatrix} \\lambda_1 \\\\ \\vdots \\\\ \\lambda_m \\end{bmatrix} $\n",
    "\n",
    "Then let $B = \n",
    "\\begin{bmatrix}\n",
    "b_{1,1} & b_{1,2} & \\cdots & b_{1,m} \\\\\n",
    "b_{2,1} & b_{2,2} & \\cdots & b_{2,m} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "b_{n,1} & b_{m,2} & \\cdots & b_{n,m} \n",
    "\\end{bmatrix}$  and $\\lambda =  \\begin{bmatrix} \\lambda_1 \\\\ \\vdots \\\\ \\lambda_m \\end{bmatrix} $\n",
    "\n",
    "We end up: $\\pi_U(x)=B\\lambda$\n",
    "\n",
    "\n",
    "Now remember that $\\pi_U(x)$ must be orthogonal to all $b_i$:\n",
    "\n",
    "$$ <b_1, x-\\pi_U(x)>  = b_1^T(x-\\pi_U(x)) = 0 $$\n",
    "$$ \\vdots $$\n",
    "$$ <b_m, x-\\pi_U(x)>  = b_m^T(x-\\pi_U(x)) = 0 $$\n",
    "\n",
    "Also could be expressed as:\n",
    "\n",
    "$ \\begin{bmatrix} b_1^T \\\\ \\vdots \\\\ b_m^T \\end{bmatrix} (x-\\pi_U(x)) = \\begin{bmatrix} 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} $\n",
    "\n",
    "$\\iff$\n",
    "\n",
    "$B^T (x-\\pi_U(x)) =  \\begin{bmatrix} 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "$\\iff$\n",
    "\n",
    "$B^T (x-B\\lambda) =  \\begin{bmatrix} 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "$\\iff$\n",
    "\n",
    "$B^T x- B^TB \\lambda =  \\begin{bmatrix} 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "$\\iff$\n",
    "\n",
    "$B^TB = B^T x$\n",
    "\n",
    "$\\iff$\n",
    "\n",
    "$\\lambda = (B^TB)^{-1} B^T x$      Remember: $(B^TB)^{-1} B^T$   is the pseudo-inverse\n",
    "\n",
    "We conclude:\n",
    "\n",
    "$ P_{\\pi_U} = (B^TB)^{-1} B^T$      Remember: $(B^TB)^{-1} B^T$   is the pseudo-inverse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "a895399ee74c9bb8c2f84b506e1022159283bd55c0aac0147faf0c85b806a476"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
